{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR53lePHuiP-"
      },
      "source": [
        "# Finetune PaliGemma\n",
        "\n",
        "> *These models and code are not official Google products and were trained and released for research purposes.*\n",
        "\n",
        "\n",
        "**This notebook shows how to finetune PaliGemma 2 on a vision-language task.**\n",
        "The training data consists of 90 pairs of images and long captions describing them.\n",
        "To make it runnable on a T4 colab runtime with 16GB HBM and 12GB RAM, we opt to only finetune the attention layers of the language model and freeze the other parameters.\n",
        "\n",
        " **This setup is illustrative**. In a real usecase, the amount of data, trainable parameters, training steps and hyper-parameters and obtained results could be significantly different.\n",
        "\n",
        "This notebook uses the model reference implementation from [big_vision](https://github.com/google-research/big_vision).\n",
        "and shows how to:\n",
        "\n",
        " * Install deps, download model checkpoint and training data.\n",
        " * Load the model onto GPU devices.\n",
        " * Prepare the input to the model for training and inference.\n",
        " * Finetune the model and inspect output in validation split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U0QUFveqSP2"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DfxKb3F839Ks"
      },
      "outputs": [],
      "source": [
        "# @title Fetch big_vision code and install dependencies.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# TPUs with\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "  raise \"It seems you are using Colab with remote TPUs which is not supported.\"\n",
        "\n",
        "# Fetch big_vision repository if python doesn't know about it and install\n",
        "# dependencies needed for this notebook.\n",
        "if not os.path.exists(\"big_vision_repo\"):\n",
        "  !git clone --quiet --branch=main --depth=1 \\\n",
        "     https://github.com/google-research/big_vision big_vision_repo\n",
        "\n",
        "# Append big_vision code to python import path\n",
        "if \"big_vision_repo\" not in sys.path:\n",
        "  sys.path.append(\"big_vision_repo\")\n",
        "\n",
        "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
        "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azmRZvgGyhAb"
      },
      "source": [
        "### Configure your API key to access Kaggle\n",
        "\n",
        "To use PaliGemma, you must provide your Kaggle username and a Kaggle API key.\n",
        "\n",
        "1. To generate a Kaggle API key, go to the **Account** tab of your Kaggle user profile and select **Create New Token**. This will trigger the download of a `kaggle.json` file containing your API credentials.\n",
        "1. In Colab, select **Secrets** (ðŸ”‘) in the left pane and add your Kaggle username and Kaggle API key. Store your username under the name `KAGGLE_USERNAME` and your API key under the name `KAGGLE_KEY`.\n",
        "\n",
        "To be able to download, you will also need to acknowledge the Terms and Conditions of the PaliGemma on:\n",
        "\n",
        "* https://www.kaggle.com/models/google/paligemma/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zGLIp1Cx3_CX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate or make your credentials available in ~/.kaggle/kaggle.json\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# The T4 runtime is tight on memory to finetune this model. Preallocate\n",
        "# all memory ahead of time to avoid OOM'ing due to fragmentation.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gQNOTfF24AV4"
      },
      "outputs": [],
      "source": [
        "# @title Download checkpoint, tokenizer and dataset to local filesystem.\n",
        "#\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Use these for PaliGemma-2 3B 224pxÂ²\n",
        "LLM_VARIANT = \"gemma2_2b\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/paligemma_assets/paligemma2-3b-pt-224.b16.npz\"\n",
        "KAGGLE_HANDLE = \"google/paligemma-2/jax/paligemma2-3b-pt-224\"  # Path to fetch from Kaggle.\n",
        "\n",
        "\n",
        "# Use these for PaliGemma 1:\n",
        "# LLM_VARIANT = \"gemma_2b\"\n",
        "# MODEL_PATH = \"./paligemma-3b-pt-224.f16.npz\"\n",
        "# KAGGLE_HANDLE = \"google/paligemma/jax/paligemma-3b-pt-224\"\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "  print(\"Downloading the checkpoint from Kaggle, this could take a few minutes....\")\n",
        "  MODEL_PATH = kagglehub.model_download(KAGGLE_HANDLE, MODEL_PATH)\n",
        "  print(f\"Model path: {MODEL_PATH}\")\n",
        "\n",
        "TOKENIZER_PATH = \"/content/drive/MyDrive/paligemma_assets/paligemma_tokenizer.model\"\n",
        "if not os.path.exists(TOKENIZER_PATH):\n",
        "  print(\"Downloading the model tokenizer...\")\n",
        "  !gsutil cp gs://big_vision/paligemma_tokenizer.model {TOKENIZER_PATH}\n",
        "  print(f\"Tokenizer path: {TOKENIZER_PATH}\")\n",
        "\n",
        "# DATA_DIR=\"./longcap100\"\n",
        "# if not os.path.exists(DATA_DIR):\n",
        "#   print(\"Downloading the dataset...\")\n",
        "#   !gsutil -m -q cp -n -r gs://longcap100/ .\n",
        "#   print(f\"Data path: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDoq0O77GF30"
      },
      "source": [
        "## Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTfe2k8J4Bw0",
        "outputId": "d929f3d4-e43e-4a2b-f8ba-53fec35351a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version:  0.5.3\n",
            "JAX platform: gpu\n",
            "JAX devices:  1\n"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "import functools\n",
        "import html\n",
        "import io\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import ml_collections\n",
        "\n",
        "import tensorflow as tf\n",
        "import sentencepiece\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "from PIL import Image\n",
        "\n",
        "# Import model definition from big_vision\n",
        "from big_vision.models.proj.paligemma import paligemma\n",
        "from big_vision.trainers.proj.paligemma import predict_fns\n",
        "\n",
        "# Import big vision utilities\n",
        "import big_vision.datasets.jsonl\n",
        "import big_vision.utils\n",
        "import big_vision.sharding\n",
        "\n",
        "# Don't let TF use the GPU or TPUs\n",
        "tf.config.set_visible_devices([], \"GPU\")\n",
        "tf.config.set_visible_devices([], \"TPU\")\n",
        "\n",
        "backend = jax.extend.backend.get_backend()\n",
        "print(f\"JAX version:  {jax.__version__}\")\n",
        "print(f\"JAX platform: {backend.platform}\")\n",
        "print(f\"JAX devices:  {jax.device_count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1aghcULcEdtv"
      },
      "outputs": [],
      "source": [
        "# @title Construct model and load params into RAM.\n",
        "\n",
        "# Define model\n",
        "# IMPORTANT: Gemma-2 has a \"final_logits_softcap\" property, we set it to 0.0\n",
        "# for better transfer results.\n",
        "model_config = ml_collections.FrozenConfigDict({\n",
        "    \"llm\": {\"vocab_size\": 257_152, \"variant\": LLM_VARIANT, \"final_logits_softcap\": 0.0},\n",
        "    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n",
        "})\n",
        "model = paligemma.Model(**model_config)\n",
        "tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)\n",
        "\n",
        "# Load params - this can take up to 1 minute in T4 colabs.\n",
        "params = paligemma.load(None, MODEL_PATH, model_config)\n",
        "\n",
        "# Define `decode` function to sample outputs from the model.\n",
        "decode_fn = predict_fns.get_all(model)['decode']\n",
        "decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RWOdf_fw2SAO"
      },
      "outputs": [],
      "source": [
        "# @title Move params to GPU/TPU memory.\n",
        "#\n",
        "# To keep HBM usage low and fit in a T4 GPU (16GB HBM) we opt to only finetune\n",
        "# a part of the parameters. Additionally we keep the frozen params in float16\n",
        "# and cast trainable to float32.\n",
        "\n",
        "# Create a pytree mask of the trainable params.\n",
        "def is_trainable_param(name, param):  # pylint: disable=unused-argument\n",
        "  if name.startswith(\"llm/layers/attn/\"):  return True\n",
        "  if name.startswith(\"llm/\"):              return False\n",
        "  if name.startswith(\"img/\"):              return False\n",
        "  raise ValueError(f\"Unexpected param name {name}\")\n",
        "trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)\n",
        "\n",
        "#\n",
        "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
        "# be sharded across them to reduce HBM usage per device.\n",
        "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n",
        "\n",
        "data_sharding = jax.sharding.NamedSharding(\n",
        "    mesh, jax.sharding.PartitionSpec(\"data\"))\n",
        "\n",
        "params_sharding = big_vision.sharding.infer_sharding(\n",
        "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)\n",
        "\n",
        "# Yes: Some donated buffers are not usable.\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", message=\"Some donated buffers were not usable\")\n",
        "\n",
        "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
        "def maybe_cast_to_f32(params, trainable):\n",
        "  # Cast others to float16, since some GPUs don't support bf16.\n",
        "  return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n",
        "                      if m else p.astype(jnp.float16),\n",
        "                      params, trainable)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading all params in simultaneous - albeit much faster and more succinct -\n",
        "# requires more RAM than the T4 colab runtimes have by default (12GB RAM).\n",
        "# Instead we do it param by param.\n",
        "params, treedef = jax.tree.flatten(params)\n",
        "sharding_leaves = jax.tree.leaves(params_sharding)\n",
        "trainable_leaves = jax.tree.leaves(trainable_mask)\n",
        "for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n",
        "  params[idx] = big_vision.utils.reshard(params[idx], sharding)\n",
        "  params[idx] = maybe_cast_to_f32(params[idx], trainable)\n",
        "  params[idx].block_until_ready()\n",
        "params = jax.tree.unflatten(treedef, params)\n",
        "\n",
        "# Print params to show what the model is made of.\n",
        "def parameter_overview(params):\n",
        "  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
        "    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n",
        "\n",
        "print(\" == Model params == \")\n",
        "parameter_overview(params)"
      ],
      "metadata": {
        "id": "ipJehqguO3T9",
        "outputId": "246a8a66-4993-42b9-dd2f-5f8c17a0f06e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " == Model params == \n",
            "img/Transformer/encoder_norm/bias                                                (1152,)                float16\n",
            "img/Transformer/encoder_norm/scale                                               (1152,)                float16\n",
            "img/Transformer/encoderblock/LayerNorm_0/bias                                    (27, 1152)             float16\n",
            "img/Transformer/encoderblock/LayerNorm_0/scale                                   (27, 1152)             float16\n",
            "img/Transformer/encoderblock/LayerNorm_1/bias                                    (27, 1152)             float16\n",
            "img/Transformer/encoderblock/LayerNorm_1/scale                                   (27, 1152)             float16\n",
            "img/Transformer/encoderblock/MlpBlock_0/Dense_0/bias                             (27, 4304)             float16\n",
            "img/Transformer/encoderblock/MlpBlock_0/Dense_0/kernel                           (27, 1152, 4304)       float16\n",
            "img/Transformer/encoderblock/MlpBlock_0/Dense_1/bias                             (27, 1152)             float16\n",
            "img/Transformer/encoderblock/MlpBlock_0/Dense_1/kernel                           (27, 4304, 1152)       float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/key/bias             (27, 16, 72)           float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/key/kernel           (27, 1152, 16, 72)     float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/out/bias             (27, 1152)             float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/out/kernel           (27, 16, 72, 1152)     float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/query/bias           (27, 16, 72)           float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/query/kernel         (27, 1152, 16, 72)     float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/value/bias           (27, 16, 72)           float16\n",
            "img/Transformer/encoderblock/MultiHeadDotProductAttention_0/value/kernel         (27, 1152, 16, 72)     float16\n",
            "img/embedding/bias                                                               (1152,)                float16\n",
            "img/embedding/kernel                                                             (14, 14, 3, 1152)      float16\n",
            "img/head/bias                                                                    (2304,)                float16\n",
            "img/head/kernel                                                                  (1152, 2304)           float16\n",
            "img/pos_embedding                                                                (1, 256, 1152)         float16\n",
            "llm/embedder/input_embedding                                                     (257152, 2304)         float16\n",
            "llm/final_norm/scale                                                             (2304,)                float16\n",
            "llm/layers/attn/attn_vec_einsum/w                                                (26, 8, 256, 2304)     float32\n",
            "llm/layers/attn/kv_einsum/w                                                      (26, 2, 4, 2304, 256)  float32\n",
            "llm/layers/attn/q_einsum/w                                                       (26, 8, 2304, 256)     float32\n",
            "llm/layers/mlp/gating_einsum                                                     (26, 2, 2304, 9216)    float16\n",
            "llm/layers/mlp/linear                                                            (26, 9216, 2304)       float16\n",
            "llm/layers/post_attention_norm/scale                                             (26, 2304)             float16\n",
            "llm/layers/post_ffw_norm/scale                                                   (26, 2304)             float16\n",
            "llm/layers/pre_attention_norm/scale                                              (26, 2304)             float16\n",
            "llm/layers/pre_ffw_norm/scale                                                    (26, 2304)             float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8SRW0NuU4UcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d28c088-9e94-4641-d717-be59866df47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 885\n",
            "Training samples: 708\n",
            "Validation samples: 177\n"
          ]
        }
      ],
      "source": [
        "# # @title Define preprocess functions to create inputs to the model.\n",
        "\n",
        "# def preprocess_image(image, size=224):\n",
        "#   # Model has been trained to handle images of different aspects ratios\n",
        "#   # resized to 224x224 in the range [-1, 1]. Bilinear and antialias resize\n",
        "#   # options are helpful to improve quality in some tasks.\n",
        "#   image = np.asarray(image)\n",
        "#   if image.ndim == 2:  # Convert image without last channel into greyscale.\n",
        "#     image = np.stack((image,)*3, axis=-1)\n",
        "#   image = image[..., :3]  # Remove alpha layer.\n",
        "#   assert image.shape[-1] == 3\n",
        "\n",
        "#   image = tf.constant(image)\n",
        "#   image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)\n",
        "#   return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]\n",
        "\n",
        "# def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
        "#   # Model has been trained to handle tokenized text composed of a prefix with\n",
        "#   # full attention and a suffix with causal attention.\n",
        "#   separator = \"\\n\"\n",
        "#   tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
        "#   mask_ar = [0] * len(tokens)    # 0 to use full attention for prefix.\n",
        "#   mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.\n",
        "\n",
        "#   if suffix:\n",
        "#     suffix = tokenizer.encode(suffix, add_eos=True)\n",
        "#     tokens += suffix\n",
        "#     mask_ar += [1] * len(suffix)    # 1 to use causal attention for suffix.\n",
        "#     mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.\n",
        "\n",
        "#   mask_input = [1] * len(tokens)    # 1 if its a token, 0 if padding.\n",
        "#   if seqlen:\n",
        "#     padding = [0] * max(0, seqlen - len(tokens))\n",
        "#     tokens = tokens[:seqlen] + padding\n",
        "#     mask_ar = mask_ar[:seqlen] + padding\n",
        "#     mask_loss = mask_loss[:seqlen] + padding\n",
        "#     mask_input = mask_input[:seqlen] + padding\n",
        "\n",
        "#   return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
        "\n",
        "# def postprocess_tokens(tokens):\n",
        "#   tokens = tokens.tolist()  # np.array to list[int]\n",
        "#   try:  # Remove tokens at and after EOS if any.\n",
        "#     eos_pos = tokens.index(tokenizer.eos_id())\n",
        "#     tokens = tokens[:eos_pos]\n",
        "#   except ValueError:\n",
        "#     pass\n",
        "#   return tokenizer.decode(tokens)\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "# The path to your dataset in Google Drive\n",
        "DATASET_PATH = '/content/drive/MyDrive/datasets/Cornell_Grasp_Kaggle'\n",
        "\n",
        "def find_data_pairs(dataset_path):\n",
        "    \"\"\"\n",
        "    Scans the dataset to find all matching pairs of image and annotation files.\n",
        "    \"\"\"\n",
        "    data_pairs = []\n",
        "    annotation_files = glob.glob(os.path.join(dataset_path, '**', '*cpos.txt'), recursive=True)\n",
        "\n",
        "    for ann_path in annotation_files:\n",
        "        img_path = ann_path.replace('cpos.txt', 'r.png')\n",
        "        if os.path.exists(img_path):\n",
        "            data_pairs.append((img_path, ann_path))\n",
        "\n",
        "    return data_pairs\n",
        "\n",
        "# Create the master list of data\n",
        "all_pairs = find_data_pairs(DATASET_PATH)\n",
        "random.shuffle(all_pairs)\n",
        "\n",
        "# Split the data\n",
        "split_index = int(len(all_pairs) * 0.80)\n",
        "train_pairs = all_pairs[:split_index]\n",
        "val_pairs = all_pairs[split_index:]\n",
        "\n",
        "print(f\"Total samples: {len(all_pairs)}\")\n",
        "print(f\"Training samples: {len(train_pairs)}\")\n",
        "print(f\"Validation samples: {len(val_pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "whzWOojGOtzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ccbb79-58f5-410b-f1b4-6c78c6a20560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data processing pipeline functions are now defined.\n"
          ]
        }
      ],
      "source": [
        "# # @title Function to iterate over train and validation examples.\n",
        "# SEQLEN = 128\n",
        "\n",
        "# # TODO: Consider data iterators skipping big_vision and tf.data?\n",
        "# train_dataset = big_vision.datasets.jsonl.DataSource(\n",
        "#     os.path.join(DATA_DIR, \"data_train90.jsonl\"),\n",
        "#     fopen_keys={\"image\": DATA_DIR})\n",
        "\n",
        "# val_dataset = big_vision.datasets.jsonl.DataSource(\n",
        "#     os.path.join(DATA_DIR, \"data_val10.jsonl\"),\n",
        "#     fopen_keys={\"image\": DATA_DIR})\n",
        "\n",
        "\n",
        "# def train_data_iterator():\n",
        "#   \"\"\"Never ending iterator over training examples.\"\"\"\n",
        "#   # Shuffle examples and repeat so one can train for many epochs.\n",
        "#   dataset = train_dataset.get_tfdata().shuffle(1_000).repeat()\n",
        "#   for example in dataset.as_numpy_iterator():\n",
        "#     image = Image.open(io.BytesIO(example[\"image\"]))\n",
        "#     image = preprocess_image(image)\n",
        "\n",
        "#     prefix = \"caption en\"  # Could also be a different prefix per example.\n",
        "#     suffix = example[\"suffix\"].decode().lower()\n",
        "#     tokens, mask_ar, mask_loss, _ = preprocess_tokens(prefix, suffix, SEQLEN)\n",
        "\n",
        "#     yield {\n",
        "#         \"image\": np.asarray(image),\n",
        "#         \"text\": np.asarray(tokens),\n",
        "#         \"mask_ar\": np.asarray(mask_ar),\n",
        "#         \"mask_loss\": np.asarray(mask_loss),\n",
        "#     }\n",
        "\n",
        "\n",
        "# def validation_data_iterator():\n",
        "#   \"\"\"Single iterator over validation examples.\"\"\"\n",
        "#   for example in val_dataset.get_tfdata(ordered=True).as_numpy_iterator():\n",
        "#     image = Image.open(io.BytesIO(example[\"image\"]))\n",
        "#     image = preprocess_image(image)\n",
        "\n",
        "#     prefix = \"caption en\"  # Could also be a different prefix per example.\n",
        "#     tokens, mask_ar, _, mask_input = preprocess_tokens(prefix, seqlen=SEQLEN)\n",
        "\n",
        "#     yield {\n",
        "#         \"image\": np.asarray(image),\n",
        "#         \"text\": np.asarray(tokens),\n",
        "#         \"mask_ar\": np.asarray(mask_ar),\n",
        "#         \"mask_input\": np.asarray(mask_input),\n",
        "#     }\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "\n",
        "# This cell assumes the 'tokenizer' variable (the real SentencePieceProcessor)\n",
        "# has been loaded in a previous cell from your notebook.\n",
        "\n",
        "# Using the smaller, memory-efficient sequence length\n",
        "SEQLEN = 32\n",
        "\n",
        "def preprocess_image(image, size=224):\n",
        "    \"\"\"Resizes and normalizes the image.\"\"\"\n",
        "    image = np.asarray(image)\n",
        "    if image.ndim == 2: image = np.stack((image,)*3, axis=-1)\n",
        "    image = image[..., :3]\n",
        "    image = tf.constant(image)\n",
        "    image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)\n",
        "    return image.numpy() / 127.5 - 1.0\n",
        "\n",
        "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
        "    \"\"\"Converts text into token IDs and all necessary masks for the model.\"\"\"\n",
        "    separator = \"\\n\"\n",
        "    tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
        "    mask_ar = [0] * len(tokens)\n",
        "    mask_loss = [0] * len(tokens)\n",
        "    if suffix:\n",
        "        suffix_tokens = tokenizer.encode(suffix, add_eos=True)\n",
        "        tokens += suffix_tokens\n",
        "        mask_ar += [1] * len(suffix_tokens)\n",
        "        mask_loss += [1] * len(suffix_tokens)\n",
        "    mask_input = [1] * len(tokens)\n",
        "    if seqlen:\n",
        "        padding_len = max(0, seqlen - len(tokens))\n",
        "        padding = [tokenizer.pad_id()] * padding_len\n",
        "        tokens = tokens[:seqlen] + padding\n",
        "        mask_ar = mask_ar[:seqlen] + padding\n",
        "        mask_loss = mask_loss[:seqlen] + padding\n",
        "        mask_input = mask_input[:seqlen] + ([0] * padding_len)\n",
        "    return np.array(tokens), np.array(mask_ar), np.array(mask_loss), np.array(mask_input)\n",
        "\n",
        "def process_grasp_file_to_bbox_tokens(file_path):\n",
        "    \"\"\"Converts a grasp file into a list of bounding box token strings.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f: lines = [line.strip() for line in f.readlines()]\n",
        "    except FileNotFoundError: return []\n",
        "    bbox_token_strings = []\n",
        "    for i in range(0, len(lines), 4):\n",
        "        points_str = lines[i:i+4]\n",
        "        if len(points_str) == 4:\n",
        "            try:\n",
        "                corners = np.array([p.split() for p in points_str], dtype=float)\n",
        "                if np.isnan(corners).any():\n",
        "                    continue\n",
        "                x_coords, y_coords = corners[:, 0], corners[:, 1]\n",
        "                x_min, y_min = int(np.min(x_coords)), int(np.min(y_coords))\n",
        "                x_max, y_max = int(np.max(x_coords)), int(np.max(y_coords))\n",
        "                token_str = (f\"<loc_{y_min:04d}><loc_{x_min:04d}><loc_{y_max:04d}><loc_{x_max:04d}>\")\n",
        "                bbox_token_strings.append(token_str)\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "    return bbox_token_strings\n",
        "\n",
        "def cornell_grasp_iterator(data_pairs, is_training=True):\n",
        "    \"\"\"The main data iterator, yielding all required masks.\"\"\"\n",
        "    while True:\n",
        "        if is_training:\n",
        "            random.shuffle(data_pairs)\n",
        "        for img_path, ann_path in data_pairs:\n",
        "            try:\n",
        "                image = Image.open(img_path)\n",
        "                bbox_tokens_list = process_grasp_file_to_bbox_tokens(ann_path)\n",
        "                if not bbox_tokens_list: continue\n",
        "                selected_bbox = random.choice(bbox_tokens_list)\n",
        "                processed_image = preprocess_image(image)\n",
        "                prefix = \"detect grasp\"\n",
        "                suffix = selected_bbox\n",
        "                tokens, mask_ar, mask_loss, mask_input = preprocess_tokens(prefix, suffix, SEQLEN)\n",
        "                yield {\n",
        "                    \"image\": processed_image,\n",
        "                    \"text\": tokens,\n",
        "                    \"mask_ar\": mask_ar,\n",
        "                    \"mask_loss\": mask_loss,\n",
        "                    \"mask_input\": mask_input,\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping file due to error: {e}, Path: {img_path}\")\n",
        "                continue\n",
        "\n",
        "print(\"âœ… Data processing pipeline functions are now defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 # Import the OpenCV library\n",
        "import re\n",
        "import os\n",
        "\n",
        "def parse_bbox_from_string(caption_str):\n",
        "    \"\"\"Extracts bounding box coordinates from the model's token string output.\"\"\"\n",
        "    numbers = re.findall(r'<loc_(\\d+)>', caption_str)\n",
        "    if len(numbers) == 4:\n",
        "        coords = [int(n) for n in numbers]\n",
        "        return tuple(coords) # (ymin, xmin, ymax, xmax)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def postprocess_tokens(tokens):\n",
        "    \"\"\"Converts token IDs back to a string.\"\"\"\n",
        "    tokens = tokens.tolist()\n",
        "    try:\n",
        "        eos_pos = tokens.index(tokenizer.eos_id())\n",
        "        tokens = tokens[:eos_pos]\n",
        "    except ValueError:\n",
        "        pass\n",
        "    return tokenizer.decode(tokens)\n",
        "\n",
        "def save_eval_image(image, caption, filename):\n",
        "    \"\"\"\n",
        "    Saves an evaluation image with its predicted bounding box drawn on it.\n",
        "    \"\"\"\n",
        "    # Convert from [-1,1] to [0,255] and ensure it's in RGB format for saving\n",
        "    image_display = ((image + 1) / 2 * 255).astype(np.uint8)\n",
        "    image_to_save = image_display.copy()\n",
        "\n",
        "    # Try to parse the bounding box from the caption string\n",
        "    bbox = parse_bbox_from_string(caption)\n",
        "\n",
        "    if bbox:\n",
        "        ymin, xmin, ymax, xmax = bbox\n",
        "        pt1 = (xmin, ymin)\n",
        "        pt2 = (xmax, ymax)\n",
        "        # Draw a RED rectangle using OpenCV (color is BGR format)\n",
        "        cv2.rectangle(image_to_render, pt1, pt2, color=(0, 0, 255), thickness=2)\n",
        "\n",
        "    # Convert from RGB (used by PIL/TF) to BGR (used by OpenCV) for saving\n",
        "    image_bgr = cv2.cvtColor(image_to_render, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(filename, image_bgr)\n",
        "\n",
        "print(\"âœ… Helper functions for saving evaluation images are now defined.\")"
      ],
      "metadata": {
        "id": "ugE4lWy0sqlY",
        "outputId": "415f0027-e94b-4519-fd4e-05d775a2e14f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Evaluation and rendering helper functions are now defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dwUV_imW3WQJ"
      },
      "outputs": [],
      "source": [
        "# @title Define the training step and evaluation loop.\n",
        "#\n",
        "# The main update_fn using simple SGD.\n",
        "#\n",
        "@functools.partial(jax.jit, donate_argnums=(0,))\n",
        "def update_fn(params, batch, learning_rate):\n",
        "  imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n",
        "\n",
        "  def loss_fn(params):\n",
        "    text_logits, _ = model.apply({\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)\n",
        "    logp = jax.nn.log_softmax(text_logits, axis=-1)\n",
        "\n",
        "    # The model takes as input txts[:, :-1] but the loss is defined as predicting\n",
        "    # next tokens txts[:, 1:]. Additionally, mask_loss[:, 1:] indicates which tokens\n",
        "    # are part of the loss (e.g. prefix and padded tokens are not included).\n",
        "    mask_loss = batch[\"mask_loss\"][:, 1:]\n",
        "    targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n",
        "\n",
        "    # Compute the loss per example. i.e. the mean of per token pplx.\n",
        "    # Since each example has a different number of tokens we normalize it.\n",
        "    token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.\n",
        "    example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.\n",
        "    example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.\n",
        "\n",
        "    # batch_loss: mean of per example loss.\n",
        "    return jnp.mean(example_loss)\n",
        "\n",
        "  loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "  # Apply gradients to trainable params using SGD.\n",
        "  def apply_grad(param, gradient, trainable):\n",
        "    if not trainable: return param\n",
        "    return param - learning_rate * gradient\n",
        "\n",
        "  params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)\n",
        "\n",
        "  return params, loss\n",
        "\n",
        "# Evaluation/inference loop.\n",
        "def make_predictions(data_iterator, *, num_examples=None,\n",
        "                     batch_size=4, seqlen=SEQLEN, sampler=\"greedy\"):\n",
        "  outputs = []\n",
        "  while True:\n",
        "    # Construct a list of examples in the batch.\n",
        "    examples = []\n",
        "    try:\n",
        "      for _ in range(batch_size):\n",
        "        examples.append(next(data_iterator))\n",
        "        examples[-1][\"_mask\"] = np.array(True)  # Indicates true example.\n",
        "    except StopIteration:\n",
        "      if len(examples) == 0:\n",
        "        return outputs\n",
        "\n",
        "    # Not enough examples to complete a batch. Pad by repeating last example.\n",
        "    while len(examples) % batch_size:\n",
        "      examples.append(dict(examples[-1]))\n",
        "      examples[-1][\"_mask\"] = np.array(False)  # Indicates padding example.\n",
        "\n",
        "    # Convert list of examples into a dict of np.arrays and load onto devices.\n",
        "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
        "    batch = big_vision.utils.reshard(batch, data_sharding)\n",
        "\n",
        "    # Make model predictions\n",
        "    tokens = decode({\"params\": params}, batch=batch,\n",
        "                    max_decode_len=seqlen, sampler=sampler)\n",
        "\n",
        "    # Fetch model predictions to device and detokenize.\n",
        "    tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n",
        "    tokens = tokens[mask]  # remove padding examples.\n",
        "    responses = [postprocess_tokens(t) for t in tokens]\n",
        "\n",
        "    # Append to html output.\n",
        "    for example, response in zip(examples, responses):\n",
        "      outputs.append((example[\"image\"], response))\n",
        "      if num_examples and len(outputs) >= num_examples:\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "067wj_6bZAG3",
        "outputId": "cffd7f04-74e7-492f-d30e-f42bb3e2b29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:   1/708 | Learning Rate: 0.00042 | Loss: 2.8888\n",
            "\n",
            "--- Running evaluation at step 1 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"display: inline-block; margin: 10px;\">data:image/jpeg;base64,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</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"display: inline-block; margin: 10px;\">data:image/jpeg;base64,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</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"display: inline-block; margin: 10px;\">data:image/jpeg;base64,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</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"display: inline-block; margin: 10px;\">data:image/jpeg;base64,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</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluation complete ---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # @title Run training loop.\n",
        "# #\n",
        "# # Run a short training loop with cosine learning rate schedule.\n",
        "# #\n",
        "# # Note: the first step can be quite slow on some machines (up to several minutes)\n",
        "# # due to XLA compilation of the jax.jit'd function.\n",
        "# #\n",
        "# %%time\n",
        "\n",
        "# BATCH_SIZE = 8\n",
        "# TRAIN_EXAMPLES = 512\n",
        "# LEARNING_RATE = 0.03\n",
        "\n",
        "# TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
        "# EVAL_STEPS = TRAIN_STEPS // 4\n",
        "\n",
        "# train_data_it = train_data_iterator()\n",
        "\n",
        "# sched_fn = big_vision.utils.create_learning_rate_schedule(\n",
        "#     total_steps=TRAIN_STEPS+1, base=LEARNING_RATE,\n",
        "#     decay_type=\"cosine\", warmup_percent=0.10)\n",
        "\n",
        "# for step in range(1, TRAIN_STEPS+1):\n",
        "#   # Make list of N training examples.\n",
        "#   examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
        "\n",
        "#   # Convert list of examples into a dict of np.arrays and load onto devices.\n",
        "#   batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
        "#   batch = big_vision.utils.reshard(batch, data_sharding)\n",
        "\n",
        "#   # Training step and report training loss\n",
        "#   learning_rate = sched_fn(step)\n",
        "#   params, loss = update_fn(params, batch, learning_rate)\n",
        "\n",
        "#   loss = jax.device_get(loss)\n",
        "#   print(f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}\")\n",
        "\n",
        "#   if step == 1 or (step % EVAL_STEPS) == 0:\n",
        "#     print(f\"Model predictions at step {step}\")\n",
        "#     html_out = \"\"\n",
        "#     for image, caption in make_predictions(\n",
        "#         validation_data_iterator(), num_examples=4, batch_size=4):\n",
        "#       html_out += render_example(image, caption)\n",
        "#     display(HTML(html_out))\n",
        "\n",
        "import jax\n",
        "import big_vision.utils\n",
        "import glob\n",
        "\n",
        "# This cell assumes variables like 'params', 'update_fn', etc., are loaded from the original notebook.\n",
        "\n",
        "print(\"ðŸš€ Starting the fine-tuning process...\")\n",
        "\n",
        "# Set training parameters\n",
        "BATCH_SIZE = 4\n",
        "TRAIN_EXAMPLES = len(train_pairs) * 2\n",
        "LEARNING_RATE = 0.03\n",
        "TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
        "EVAL_STEPS = TRAIN_STEPS // 5\n",
        "\n",
        "# Create our custom data iterators\n",
        "train_data_it = cornell_grasp_iterator(train_pairs, is_training=True)\n",
        "val_data_it = cornell_grasp_iterator(val_pairs, is_training=False)\n",
        "\n",
        "# Set up learning rate schedule\n",
        "lr_sched_fn = big_vision.utils.create_learning_rate_schedule(\n",
        "    total_steps=TRAIN_STEPS + 1, base=LEARNING_RATE,\n",
        "    decay_type=\"cosine\", warmup_percent=0.10)\n",
        "\n",
        "# Run the main training loop\n",
        "for step in range(1, TRAIN_STEPS + 1):\n",
        "    examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
        "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
        "    batch = big_vision.utils.reshard(batch, data_sharding)\n",
        "\n",
        "    learning_rate = lr_sched_fn(step)\n",
        "    params, loss = update_fn(params, batch, learning_rate)\n",
        "\n",
        "    if step % 20 == 0 or step == 1 or step == TRAIN_STEPS:\n",
        "      print(f\"Step: {step:3d}/{TRAIN_STEPS} | Learning Rate: {learning_rate:.5f} | Loss: {loss:.4f}\")\n",
        "\n",
        "    # Run periodic evaluation on the validation set\n",
        "    if step == 1 or (step % EVAL_STEPS) == 0 or step == TRAIN_STEPS:\n",
        "        print(f\"\\n--- Running evaluation at step {step} ---\")\n",
        "\n",
        "        # Clean up old evaluation images\n",
        "        for f in glob.glob('/content/eval_*.jpg'):\n",
        "            os.remove(f)\n",
        "\n",
        "        # Generate and save new evaluation images\n",
        "        saved_files = []\n",
        "        for i, (image, caption) in enumerate(make_predictions(\n",
        "            val_data_it, num_examples=4, batch_size=BATCH_SIZE)):\n",
        "\n",
        "            filename = f\"/content/eval_step_{step}_img_{i}.jpg\"\n",
        "            save_eval_image(image, caption, filename)\n",
        "            saved_files.append(filename)\n",
        "\n",
        "        print(f\"âœ… Evaluation complete. Images saved to:\")\n",
        "        for f in saved_files:\n",
        "            print(f\"  - {f}\")\n",
        "        print(\"\\n--> Please check the file browser on the left to view images.\\n\")\n",
        "\n",
        "print(\"\\nâœ… Fine-tuning complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai0NMbAwsr0j"
      },
      "source": [
        "# Save the final checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H_3CV33_JkV"
      },
      "outputs": [],
      "source": [
        "def npsave(pytree, path):\n",
        "  names_and_vals, _ = big_vision.utils.tree_flatten_with_names(pytree)\n",
        "  with open(path, \"wb\") as f:\n",
        "    np.savez(f, **{k:v for k, v in names_and_vals})\n",
        "\n",
        "# Takes around 4 minutes\n",
        "npsave(params, 'my-custom-paligemma-ckpt.npz')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}